{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "100977df",
   "metadata": {},
   "source": [
    "# [E-04] 작사가 인공지능 만들기\n",
    "\n",
    "이번 프로젝트는 순환신경망(RNN)을 이용하여 단어를 생성하고 문장을 만들어내는 모델을 만드는 것을 목표로 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b34817",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 1. 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "070ef3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " [\"Now I've heard there was a secret chord\", 'That David played, and it pleased the Lord', \"But you don't really care for music, do you?\"]\n"
     ]
    }
   ],
   "source": [
    "# 라이브러리 불러오기\n",
    "import tensorflow as tf\n",
    "import os ,re\n",
    "import glob\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus에 담기\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c16595b",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 2. 데이터 정제하기\n",
    "\n",
    "preprocess_sentence() 함수를 사용하여 데이터를 정제하는 작업을 진행할 예정이다. 정제하는 과정에서 사용된 코드의 순서는 다음과 같다.\n",
    "\n",
    "1. 소문자로 바꾸고, 양쪽 공백 지우기\n",
    "2. 특수문자 양쪽에 공백 넣기\n",
    "3. 여러개의 공백은 하나의 공백으로 바꾸기\n",
    "4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꾸기\n",
    "5. 다시 양쪽 공백을 지우기\n",
    "6. 문장 시작에는 <start>, 끝에는 <end>를 추가하기\n",
    "    \n",
    "토큰의 개수가 15개를 넘어가는 문장은 학습 데이터에서 제외시킨다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18b35a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() \n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) \n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) \n",
    "    sentence = sentence.strip() \n",
    "    sentence = '<start> ' + sentence + ' <end>'\n",
    "    return sentence\n",
    "\n",
    "# 필터링이 제대로 작동하는지 확인\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "607afda6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13665"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 원하지 않는 문장은 건너뛰고 정제하여 corpus에 담기\n",
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue \n",
    "    if sentence[-1] == \":\": continue\n",
    "    if sentence[0] == \"[\": continue\n",
    "    if len(sentence) > 15 : continue\n",
    "    \n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "        \n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0030c364",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 3. 토큰화하기\n",
    "\n",
    "tensorflow의 Tokenizer와 pad_sequences를 사용하여 토큰화를 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcacb43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2 159   3 ...   0   0   0]\n",
      " [  2 159   3 ...   0   0   0]\n",
      " [  2  54 864 ...   0   0   0]\n",
      " ...\n",
      " [  2 213 543 ...   0   0   0]\n",
      " [  2  59  17 ...   0   0   0]\n",
      " [  2  35  21 ...   0   0   0]] <keras_preprocessing.text.Tokenizer object at 0x7f9d143d4850>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=15000,    # tokenizer가 기억할 수 있는 단어 수\n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"   # 10000개의 단어에 포함되지 못하면 <unk>로 표시\n",
    "    )\n",
    "\n",
    "    tokenizer.fit_on_texts(corpus)                      # corpus를 이용해 tokenizer 내부의 단어장을 완성\n",
    "\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)       # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환\n",
    "    \n",
    "    # 문장 뒤에 패딩을 넣어 maxlen에 시퀀스 길이 맞춤\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad3a8ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : i\n",
      "5 : ,\n",
      "6 : .\n",
      "7 : you\n",
      "8 : oh\n",
      "9 : it\n",
      "10 : me\n"
     ]
    }
   ],
   "source": [
    "# 저장된 단어의 index 10개까지 확인\n",
    "\n",
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed613c0",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 4. 데이터셋 분리하기\n",
    "\n",
    "tensor에서 마지막 토큰을 잘라낸 것을 소스 문장으로, tensor에서 <start>를 잘라낸 것을 타겟 문장으로 한다.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7102cbe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2 159   3   0   0   0   0   0   0   0   0   0]\n",
      "[159   3   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1]  \n",
    "tgt_input = tensor[:, 1:]    \n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7a77bf5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (10932, 12) Source Validation: (2733, 12)\n",
      "Target Train: (10932, 12) Target Validation: (2733, 12)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input,\n",
    "                                                         tgt_input,\n",
    "                                                         test_size=0.2,\n",
    "                                                         shuffle=True)\n",
    "print(\"Source Train:\", enc_train.shape, \"Source Validation:\", enc_val.shape)\n",
    "print(\"Target Train:\", dec_train.shape, \"Target Validation:\", dec_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088cf629",
   "metadata": {},
   "source": [
    "tf.data.Dataset.from_tensor_slices()를 이용해 corpus 텐서를 tf.data.Dataset객체로 변환하는 작업을 해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b03f898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 12), (256, 12)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    " # tokenizer가 구축한 단어 10000개에 <pad>를 포함하여 10001개\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b4519e",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 5. 모델 설계하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c1af255",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 512\n",
    "hidden_size = 2048\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e6b80e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 12, 15001), dtype=float32, numpy=\n",
       "array([[[ 8.0148056e-06,  1.6628644e-04, -4.3495620e-05, ...,\n",
       "          1.9002553e-04,  3.7800567e-04,  2.5659814e-04],\n",
       "        [ 1.9439929e-06,  6.6383678e-04,  2.1686821e-05, ...,\n",
       "          4.0540379e-04,  7.9255371e-04,  5.3634867e-04],\n",
       "        [-4.6138381e-04,  1.0157702e-03,  1.2258209e-04, ...,\n",
       "          3.4196037e-04,  6.4837979e-04,  5.5305817e-04],\n",
       "        ...,\n",
       "        [-2.1802976e-03,  2.4761588e-03, -7.2576036e-04, ...,\n",
       "         -5.6108634e-04,  1.3960727e-03,  1.2777292e-03],\n",
       "        [-2.2277515e-03,  2.4480612e-03, -9.3464321e-04, ...,\n",
       "         -5.9112062e-04,  1.5744300e-03,  1.1794448e-03],\n",
       "        [-2.2607862e-03,  2.3669524e-03, -1.0683488e-03, ...,\n",
       "         -5.8430270e-04,  1.6991834e-03,  1.0422738e-03]],\n",
       "\n",
       "       [[ 8.0148056e-06,  1.6628644e-04, -4.3495620e-05, ...,\n",
       "          1.9002553e-04,  3.7800567e-04,  2.5659814e-04],\n",
       "        [-1.0279367e-04,  2.2296897e-04, -1.6884742e-05, ...,\n",
       "          3.8192031e-04,  3.4380399e-04,  1.5955976e-04],\n",
       "        [-4.4900025e-04,  1.4953045e-04, -2.1851745e-05, ...,\n",
       "          3.7833332e-04,  6.0722540e-04,  1.3968952e-04],\n",
       "        ...,\n",
       "        [-1.7864034e-03,  1.6248141e-03, -5.8323890e-04, ...,\n",
       "         -4.1540989e-04,  1.3895956e-03,  6.7008339e-04],\n",
       "        [-1.9202987e-03,  1.7244793e-03, -8.2533772e-04, ...,\n",
       "         -4.9896998e-04,  1.5069225e-03,  6.1365473e-04],\n",
       "        [-2.0256988e-03,  1.7548734e-03, -9.9095295e-04, ...,\n",
       "         -5.3920056e-04,  1.5780908e-03,  5.3065072e-04]],\n",
       "\n",
       "       [[ 8.0148056e-06,  1.6628644e-04, -4.3495620e-05, ...,\n",
       "          1.9002553e-04,  3.7800567e-04,  2.5659814e-04],\n",
       "        [-4.6912002e-05,  2.9578368e-04, -1.0867316e-04, ...,\n",
       "          3.3841265e-05,  7.2266377e-04,  2.1535705e-04],\n",
       "        [-3.0000433e-06,  3.3609517e-04,  2.3058737e-05, ...,\n",
       "          7.3197501e-05,  7.0698035e-04,  2.3348726e-04],\n",
       "        ...,\n",
       "        [-1.5120220e-03,  1.1479135e-03, -9.6922426e-04, ...,\n",
       "         -3.4851939e-04,  1.3061956e-03,  6.2012236e-04],\n",
       "        [-1.6461213e-03,  1.2213332e-03, -1.0753195e-03, ...,\n",
       "         -3.5586380e-04,  1.4049178e-03,  5.9455016e-04],\n",
       "        [-1.7596001e-03,  1.2486888e-03, -1.1242431e-03, ...,\n",
       "         -3.4330913e-04,  1.4700519e-03,  5.3387781e-04]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 8.0148056e-06,  1.6628644e-04, -4.3495620e-05, ...,\n",
       "          1.9002553e-04,  3.7800567e-04,  2.5659814e-04],\n",
       "        [ 8.1423452e-05,  6.4794091e-04,  2.1391961e-06, ...,\n",
       "          3.6614499e-04,  7.6322246e-04,  2.7719853e-04],\n",
       "        [ 1.4722487e-04,  6.4631121e-04,  2.4733663e-05, ...,\n",
       "          4.4021831e-04,  1.2541098e-03,  2.3475250e-04],\n",
       "        ...,\n",
       "        [-1.6634954e-03,  1.4746021e-03, -9.9589920e-04, ...,\n",
       "         -2.9566919e-04,  2.6948354e-03,  9.5109695e-05],\n",
       "        [-1.8294686e-03,  1.4437060e-03, -1.0614328e-03, ...,\n",
       "         -3.1507236e-04,  2.5717767e-03,  1.4376411e-05],\n",
       "        [-1.9684429e-03,  1.3869666e-03, -1.0809744e-03, ...,\n",
       "         -3.1008222e-04,  2.4341191e-03, -6.8805588e-05]],\n",
       "\n",
       "       [[ 8.0148056e-06,  1.6628644e-04, -4.3495620e-05, ...,\n",
       "          1.9002553e-04,  3.7800567e-04,  2.5659814e-04],\n",
       "        [ 6.7524772e-05,  3.3948451e-04, -2.4254146e-04, ...,\n",
       "          2.6234827e-04,  5.4268265e-04,  5.5153185e-04],\n",
       "        [ 2.6686824e-04,  4.8334984e-04, -4.1966719e-04, ...,\n",
       "          9.7842494e-05,  6.7201152e-04,  6.4752862e-04],\n",
       "        ...,\n",
       "        [-1.1708189e-03,  1.3956592e-03, -1.0364018e-03, ...,\n",
       "         -1.0994067e-03,  1.9297394e-03,  8.3628896e-04],\n",
       "        [-1.4063729e-03,  1.5151922e-03, -1.1598986e-03, ...,\n",
       "         -1.0828684e-03,  2.0601705e-03,  7.9615164e-04],\n",
       "        [-1.6128545e-03,  1.5770433e-03, -1.2243562e-03, ...,\n",
       "         -1.0208973e-03,  2.1254416e-03,  7.1605603e-04]],\n",
       "\n",
       "       [[ 8.0148056e-06,  1.6628644e-04, -4.3495620e-05, ...,\n",
       "          1.9002553e-04,  3.7800567e-04,  2.5659814e-04],\n",
       "        [-6.7171381e-06,  3.6562572e-04,  1.6353924e-04, ...,\n",
       "          3.1217592e-04,  8.0828782e-04,  6.5217889e-04],\n",
       "        [-6.4558008e-05,  2.7590187e-04,  2.6601722e-04, ...,\n",
       "          3.9059270e-04,  1.1834670e-03,  8.0818043e-04],\n",
       "        ...,\n",
       "        [-2.2636561e-03,  5.8342982e-04,  6.9736259e-04, ...,\n",
       "          2.8526725e-04,  2.9603217e-03,  6.7876774e-04],\n",
       "        [-2.5159391e-03,  7.9950056e-04,  3.6335347e-04, ...,\n",
       "          1.8798409e-04,  2.8826145e-03,  5.5371743e-04],\n",
       "        [-2.6900219e-03,  9.4643846e-04,  8.3934909e-05, ...,\n",
       "          1.1761847e-04,  2.7627700e-03,  4.2023248e-04]]], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋에서 한 배치만 불러오기\n",
    "\n",
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d6bed4",
   "metadata": {},
   "source": [
    "출력된 텐서의 shape를 살펴보면 shape=(256, 12, 15001)에서 배치 사이즈가 256으로 지정되어 있고 Dense 레이어의 차원수는 15001이다. 이는 15001개의 단어 중 확률이 가장 높은 단어를 찾아야 하기 때문이다. 여기서 12은 앞서 동일하게 맞춰 준 시퀀스의 길이인 max_len이 12이었다는 것을 의미한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eec5b17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  7680512   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  20979712  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  33562624  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  30737049  \n",
      "=================================================================\n",
      "Total params: 92,959,897\n",
      "Trainable params: 92,959,897\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08051b40",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 6. 모델 학습시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8041aee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "53/53 [==============================] - 25s 433ms/step - loss: 2.4713\n",
      "Epoch 2/10\n",
      "53/53 [==============================] - 23s 436ms/step - loss: 1.5002\n",
      "Epoch 3/10\n",
      "53/53 [==============================] - 22s 423ms/step - loss: 1.4126\n",
      "Epoch 4/10\n",
      "53/53 [==============================] - 22s 424ms/step - loss: 1.3717\n",
      "Epoch 5/10\n",
      "53/53 [==============================] - 23s 430ms/step - loss: 1.3395\n",
      "Epoch 6/10\n",
      "53/53 [==============================] - 23s 428ms/step - loss: 1.2959\n",
      "Epoch 7/10\n",
      "53/53 [==============================] - 23s 426ms/step - loss: 1.2491\n",
      "Epoch 8/10\n",
      "53/53 [==============================] - 23s 427ms/step - loss: 1.2045\n",
      "Epoch 9/10\n",
      "53/53 [==============================] - 23s 428ms/step - loss: 1.1608\n",
      "Epoch 10/10\n",
      "53/53 [==============================] - 23s 428ms/step - loss: 1.1208\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9c701632b0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5686271d",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 7. 작문시켜 평가하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a766db",
   "metadata": {},
   "source": [
    "단어 하나씩 예측해 문장을 만든다.\n",
    "\n",
    "1. 입력받은 문장의 텐서를 입력\n",
    "2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냄\n",
    "3. 2에서 예측된 word index를 문장 뒤에 붙임\n",
    "4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마침"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a02c5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=15):\n",
    "    # 테스트를 위해 입력받은 init_sentence도 텐서로 변환\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "    \n",
    "    while True:\n",
    "        # 1\n",
    "        predict = model(test_tensor) \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3\n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환 \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66ab27fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> oh , oh , oh <end> '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> oh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fb07b9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> it s a monster <end> '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2ae7567",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> but i m not <end> '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> but\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a99fe4af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i m a survivor <end> '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e7c1cc",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# <프로젝트 회고>\n",
    "\n",
    "이번 프로젝트는 좋은 모델을 만들기 위해 수정해가는 과정에서 시간이 정말 많이 걸렸다. epoch는 10으로 두고 loss를 최대한 낮추려고 노력을 했다. 많은 시도를 해 본 결과, embedding_size = 512, hidden_size = 2048 일 때 성능이 좋게 나왔다. loss는 1.12까지 줄어들었고 파라미터의 수는 약 9,300만개였다. 출력된 결과물을 보니 짧긴 하지만 문장이 만들어졌다. 문장이 서로 이어지는 것 같기도 해서 신기했다. 문장이 길어지는 것을 막고 <pad>의 갯수를 줄이기 위해 학습되는 문장들은 토큰이 15개 이하로 제한을 두었다. 하지만 막상 결과물을 확인하니 너무 짧은 문장들만 나온 것 같아 아쉬운 생각도 들었다. NLP에 대해 처음 알게 되었는데 NLP에서 기본이라 할 수 있는 RNN 모델에 대해 자세히 공부할 수 있어서 좋았다. 조금 더 공부해서 GPT-2를 활용해 더 긴 문장을 만들어보는 것도 도전해봐야겠다. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
